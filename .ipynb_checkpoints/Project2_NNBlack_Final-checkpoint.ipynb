{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8Ec4XT7Y5k5k",
    "outputId": "f30c1e22-6f75-4d1b-c057-ace8dd1cbccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joyee/Downloads/BS'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JcR_eQulfHs",
    "outputId": "780d88cb-4e0b-4b48-8fdf-6ac0b88d23eb"
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# reading training file\n",
    "df = pd.read_csv('Project2_training.csv')\n",
    "\n",
    "# normalize stock and call prices in relation to the option's strike price\n",
    "df['Stock Price'] = df['Stock Price']/df['Strike Price']\n",
    "df['Call Price']  = df['Call Price'] /df['Strike Price']\n",
    "\n",
    "# split data into training and testing sets\n",
    "n = 3000\n",
    "n_train =  (int)(0.8 * n)\n",
    "\n",
    "# select training set and define independent and dependent variable (call price)\n",
    "train = df[0:n_train]\n",
    "X_train = train[['Stock Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free']].values\n",
    "y_train = train['Call Price'].values\n",
    "\n",
    "# select testing set and define independent and dependent variable (call price)\n",
    "test = df[n_train+1:n]\n",
    "X_test = test[['Stock Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free']].values\n",
    "y_test = test['Call Price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XORKTcNGrUz4"
   },
   "source": [
    "Load Tensor Flow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N4Eno1uprXiL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17d836bbef7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# importing Keras classes to build the neural networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m \u001b[0;31m# https://keras.io/api/models/sequential/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# keras provides layers api to construct the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# to build the neural we will use Keras due to its friendly API\n",
    "# https://keras.io/api/\n",
    "\n",
    "# importing Keras classes to build the neural networks\n",
    "from keras.models import Sequential # https://keras.io/api/models/sequential/\n",
    "\n",
    "# keras provides layers api to construct the neural network\n",
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU, ReLU #https://keras.io/api/layers/\n",
    "\n",
    "from keras import backend\n",
    "def custom_activation(x):\n",
    "    return backend.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMdOG73dG5S6"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import random\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHQ9v4L-yVMi"
   },
   "source": [
    "Single Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nvFqr4ZFtjsk",
    "outputId": "720fcd03-1ac0-4c33-e11b-5e366fc7ae9c"
   },
   "outputs": [],
   "source": [
    "# for this NN we will define it with 300 neurons\n",
    "nodes = 300\n",
    "# initialize the mode as Sequential class - the next steps we will add layers to it \n",
    "model = Sequential()\n",
    "\n",
    "# to add a layer we call the add method from model (Sequential) and pass as argument the Dense class\n",
    "# https://keras.io/api/layers/core_layers/dense/\n",
    "model.add(Dense(nodes, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "# since we want to predict a single value we add one additional layer with one neuron \n",
    "model.add(Dense(1))\n",
    "# we will use our customized activation function\n",
    "model.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# https://keras.io/api/models/model_training_apis/\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihn4kkiasNp1"
   },
   "source": [
    "Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4R5uZxJRsQmB",
    "outputId": "1668fd90-8855-4f6b-8e37-0ebd7a0154c9"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 300 neurons - but instead of a single layer of 300 neurons, we will use 3 layers of 100 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes2 = 100\n",
    "\n",
    "# initialize our new model as sequential class\n",
    "model2 = Sequential()\n",
    "\n",
    "# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments\n",
    "model2.add(Dense(nodes2, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "# add second layer of 100 neurons - no need to declare the input dimensions, keras does that automatically\n",
    "model2.add(Dense(nodes2, activation='relu'))\n",
    "\n",
    "# add third layer of 100 neurons \n",
    "model2.add(Dense(nodes2, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model2.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rpxeOpW-_r_U",
    "outputId": "dbac489e-1e0f-4654-df68-e1f620afe8fd"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 300 neurons - we will use 6 layers of 50 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes2_2 = 50\n",
    "\n",
    "# initialize our new model as sequential class\n",
    "model2_2 = Sequential()\n",
    "\n",
    "# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments\n",
    "model2_2.add(Dense(nodes2_2, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "for i in range(5):\n",
    "  model2_2.add(Dense(nodes2_2, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model2_2.add(Dense(1))\n",
    "model2_2.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model2_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model2_2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model2_2.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiANpXLf86Fd"
   },
   "source": [
    "New NN structure with 1200 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CGz_ehj_7oy0",
    "outputId": "fd93412c-1c54-4460-e994-0a8ce5e6e6bb"
   },
   "outputs": [],
   "source": [
    "# for this NN we will define it with 600 neurons\n",
    "nodes3 = 600\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(nodes3, activation='relu', input_dim=X_train.shape[1])) \n",
    "\n",
    "# add one additional layer with one neuron \n",
    "model3.add(Dense(1))\n",
    "# we will use our customized activation function\n",
    "model3.add(Activation(custom_activation))\n",
    "\n",
    "# compile the model\n",
    "model3.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model3.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model3.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4mTApAJY_tx_",
    "outputId": "b2eae7f3-2100-437d-dbba-23d6dd67e5cb"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 600 neurons - but instead of a single layer of 600 neurons, we will use 6 layers of 100 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes4 = 100\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Dense(nodes4, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model4.add(Dense(nodes4, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model4.add(Dense(1))\n",
    "model4.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model4.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model4.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs163OitQTa8"
   },
   "source": [
    "New NN with different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1ph-1K9jQS4N",
    "outputId": "0fbb9793-e979-43c5-87fe-e0832476e830"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and sigmoid activation\n",
    "nodes6 = 300\n",
    "model6 = Sequential()\n",
    "\n",
    "\n",
    "model6.add(Dense(nodes6, activation='sigmoid', input_dim=X_train.shape[1]))\n",
    "\n",
    "model6.add(Dense(1))\n",
    "model6.add(Activation(custom_activation))\n",
    "\n",
    "model6.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model6.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model6.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1rjCCSQ_nJy5",
    "outputId": "f30ea04b-b589-467b-b19c-3cbffdb838e6"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using Sigmoid activation function\n",
    "nodes7 = 100\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(Dense(nodes7, activation='sigmoid', input_dim=X_train.shape[1])) \n",
    "\n",
    "model7.add(Dense(nodes7, activation='sigmoid'))\n",
    "model7.add(Dense(nodes7, activation='sigmoid'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model7.add(Dense(1))\n",
    "model7.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model7.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model7.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model7.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tk9BYhEI2Hvr",
    "outputId": "a9ee1cde-2a64-4a1f-f3d9-8095ad7ff5cc"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, sigmoid, relu\n",
    "nodes7_2 = 100\n",
    "model7_2 = Sequential()\n",
    "\n",
    "model7_2.add(Dense(nodes7_2, activation='sigmoid', input_dim=X_train.shape[1])) \n",
    "\n",
    "model7_2.add(Dense(nodes7_2, activation='sigmoid'))\n",
    "model7_2.add(Dense(nodes7_2, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model7_2.add(Dense(1))\n",
    "model7_2.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model7_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model7_2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model7_2.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mEA9UBLQ96lR",
    "outputId": "508bb1d9-5419-4983-9ff7-b07b20c92b0d"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, relu, relu\n",
    "nodes7_3 = 100\n",
    "model7_3 = Sequential()\n",
    "\n",
    "model7_3.add(Dense(nodes7_3, activation='sigmoid', input_dim=X_train.shape[1])) \n",
    "\n",
    "model7_3.add(Dense(nodes7_3, activation='relu'))\n",
    "model7_3.add(Dense(nodes7_3, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model7_3.add(Dense(1))\n",
    "model7_3.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model7_3.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model7_3.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model7_3.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1qZlZJH56XzS",
    "outputId": "763302be-aa2a-484b-d44f-b78d0bfbd36a"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: relu, relu, sigmoid\n",
    "nodes7_4 = 100\n",
    "model7_4 = Sequential()\n",
    "\n",
    "model7_4.add(Dense(nodes7_4, activation='relu', input_dim=X_train.shape[1])) \n",
    "\n",
    "model7_4.add(Dense(nodes7_4, activation='relu'))\n",
    "model7_4.add(Dense(nodes7_4, activation='sigmoid'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model7_4.add(Dense(1))\n",
    "model7_4.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model7_4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model7_4.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model7_4.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XggJMD8bnJ42",
    "outputId": "81bdbd33-0837-4c12-eb7d-ada05d1909f1"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and tanh activation\n",
    "nodes8 = 300\n",
    "model8 = Sequential()\n",
    "\n",
    "\n",
    "model8.add(Dense(nodes8, activation='tanh', input_dim=X_train.shape[1]))\n",
    "\n",
    "model8.add(Dense(1))\n",
    "model8.add(Activation(custom_activation))\n",
    "\n",
    "model8.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model8.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model8.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pibm0AB9nKBF",
    "outputId": "ee204dc0-4f8c-4c05-bfd3-b1eabc83c20d"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using tanh activation function\n",
    "nodes9 = 100\n",
    "model9 = Sequential()\n",
    "\n",
    "model9.add(Dense(nodes9, activation='tanh', input_dim=X_train.shape[1])) \n",
    "\n",
    "model9.add(Dense(nodes9, activation='tanh'))\n",
    "model9.add(Dense(nodes9, activation='tanh'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model9.add(Dense(1))\n",
    "model9.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model9.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model9.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model9.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8YQoqePOfMt"
   },
   "source": [
    "Models with different Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rhnzx6q-OlY2",
    "outputId": "17d15e79-69ad-4752-c8ea-e4cad02365b3"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and rmsprop optimizer\n",
    "nodes10 = 300\n",
    "model10 = Sequential()\n",
    "\n",
    "\n",
    "model10.add(Dense(nodes10, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "model10.add(Dense(1))\n",
    "model10.add(Activation(custom_activation))\n",
    "\n",
    "model10.compile(loss='mse', optimizer='rmsprop') \n",
    "\n",
    "# fit the model\n",
    "history = model10.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model10.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6RXfJB6-aH-7",
    "outputId": "47803f5d-799d-4a4d-da6a-aeba2e90d93b"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using relu activation function and rmsprop optimizer\n",
    "nodes11 = 100\n",
    "model11 = Sequential()\n",
    "\n",
    "model11.add(Dense(nodes11, activation='relu', input_dim=X_train.shape[1])) \n",
    "\n",
    "model11.add(Dense(nodes11, activation='relu'))\n",
    "model11.add(Dense(nodes11, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model11.add(Dense(1))\n",
    "model11.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model11.compile(loss='mse', optimizer='rmsprop') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model11.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model11.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FZnlRbBKbv_I",
    "outputId": "6a4694d3-a12b-4053-a8d2-426ed38d0559"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and momentum (sgd) optimizer\n",
    "nodes12 = 300\n",
    "model12 = Sequential()\n",
    "\n",
    "\n",
    "model12.add(Dense(nodes12, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "model12.add(Dense(1))\n",
    "model12.add(Activation(custom_activation))\n",
    "\n",
    "model12.compile(loss='mse', optimizer='sgd') \n",
    "\n",
    "# fit the model\n",
    "history = model12.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model12.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i86TIG6RcPz2",
    "outputId": "703bb3e1-3529-4a0e-fb60-13f058b0fb69"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using relu activation function and momentum (sgd) optimizer\n",
    "nodes13 = 100\n",
    "model13 = Sequential()\n",
    "\n",
    "model13.add(Dense(nodes13, activation='relu', input_dim=X_train.shape[1])) \n",
    "\n",
    "model13.add(Dense(nodes13, activation='relu'))\n",
    "model13.add(Dense(nodes13, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model13.add(Dense(1))\n",
    "model13.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model13.compile(loss='mse', optimizer='sgd') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model13.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model13.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuVGc2prGqIF"
   },
   "source": [
    "Testing different Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a1ZCHLiXq8qJ",
    "outputId": "0992422c-9d27-4efb-b77f-601d8614ea3d"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and adam optimizer and 500 epochs\n",
    "nodes14 = 300\n",
    "model14 = Sequential()\n",
    "\n",
    "\n",
    "model14.add(Dense(nodes14, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "model14.add(Dense(1))\n",
    "model14.add(Activation(custom_activation))\n",
    "\n",
    "model14.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model14.fit(X_train, y_train, batch_size=64, epochs=500, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model14.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0b2OVIK1rf6F",
    "outputId": "41fa6713-3464-473a-9f10-d2200c1ce039"
   },
   "outputs": [],
   "source": [
    "#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 500 epochs\n",
    "nodes15 = 100\n",
    "model15 = Sequential()\n",
    "\n",
    "model15.add(Dense(nodes15, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model15.add(Dense(nodes15, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model15.add(Dense(1))\n",
    "model15.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model15.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model15.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model15.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qldy9MJfsGBo",
    "outputId": "1e29ea75-8f65-4013-afd9-250a38db740e"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and adam optimizer and 1000 epochs\n",
    "nodes16 = 300\n",
    "model16 = Sequential()\n",
    "\n",
    "\n",
    "model16.add(Dense(nodes16, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "model16.add(Dense(1))\n",
    "model16.add(Activation(custom_activation))\n",
    "\n",
    "model16.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model16.fit(X_train, y_train, batch_size=64, epochs=1000, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model16.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bsTtEVJAsS8T",
    "outputId": "0c416d9b-278a-46bf-bb68-ec66d5f26bdc"
   },
   "outputs": [],
   "source": [
    "#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 1000 epochs\n",
    "nodes17 = 100\n",
    "model17 = Sequential()\n",
    "\n",
    "model17.add(Dense(nodes17, activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model17.add(Dense(nodes17, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model17.add(Dense(1))\n",
    "model17.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model17.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model17.fit(X_train, y_train, batch_size=512, epochs=1000, validation_split=0.1, verbose=2,\n",
    "                          validation_data=(X_test, y_test))\n",
    "print()\n",
    "print(model17.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52H6lgy7SNzP"
   },
   "source": [
    "# Divide data into Training, Validation and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA8Vdxh8SEWR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import random\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjnH-EowSdC1"
   },
   "outputs": [],
   "source": [
    "# Include option price with and without noise in data set splitting for later BS mean error calculation on test set\n",
    "X2 = df[['Stock Price', 'Risk-free','Volatility','Maturity','Dividends']]\n",
    "y2 = df[['Call Price']]\n",
    "\n",
    "\n",
    "#Divide data into training (60%), testing (20%) and validation (20%) sets\n",
    "\n",
    "# Divide data into training set and test set(note that random seed is set)\n",
    "X2_train,X2_test,y2_train,y2_test=train_test_split(X2,y2,test_size=0.4,random_state=100)\n",
    "\n",
    "# Divide training set into training and validation set\n",
    "X2_train,X2_val,y2_train,y2_val=train_test_split(X2_train,y2_train,test_size=0.5,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeNGnGJLSgZL"
   },
   "outputs": [],
   "source": [
    "# Scale features based on Z-Score\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X2_train)\n",
    "\n",
    "\n",
    "X2_scaled_train = scaler.transform(X2_train)\n",
    "X2_scaled_val = scaler.transform(X2_val)\n",
    "X2_scaled_test = scaler.transform(X2_test)\n",
    "\n",
    "y2_train = np.asarray(y2_train)\n",
    "y2_val = np.asarray(y2_val)\n",
    "y2_test = np.asarray(y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ppHV8ySmuz"
   },
   "source": [
    "Single Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "64clzBrWSiYW",
    "outputId": "470c0e18-d131-4b9c-dd1e-204ecbd1ab29"
   },
   "outputs": [],
   "source": [
    "# for this NN we will define it with 300 neurons\n",
    "nodes20 = 300\n",
    "# initialize the mode as Sequential class - the next steps we will add layers to it \n",
    "model20 = Sequential()\n",
    "\n",
    "# to add a layer we call the add method from model (Sequential) and pass as argument the Dense class\n",
    "# https://keras.io/api/layers/core_layers/dense/\n",
    "model20.add(Dense(nodes20, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "# since we want to predict a single value we add one additional layer with one neuron \n",
    "model20.add(Dense(1))\n",
    "# we will use our customized activation function\n",
    "model20.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# https://keras.io/api/models/model_training_apis/\n",
    "\n",
    "# compile the model\n",
    "model20.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "\n",
    "# fit the model\n",
    "history = model20.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model20.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9sOSNRwS7Xn"
   },
   "source": [
    "Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T3S9DWCPSksb",
    "outputId": "6c8933a9-19a7-43ac-948e-a8858b7d9363"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 300 neurons - but instead of a single layer of 300 neurons, we will use 3 layers of 100 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes21 = 100\n",
    "\n",
    "# initialize our new model as sequential class\n",
    "model21 = Sequential()\n",
    "\n",
    "# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments\n",
    "model21.add(Dense(nodes21, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "# add second layer of 100 neurons - no need to declare the input dimensions, keras does that automatically\n",
    "model21.add(Dense(nodes21, activation='relu'))\n",
    "\n",
    "# add third layer of 100 neurons \n",
    "model21.add(Dense(nodes21, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model21.add(Dense(1))\n",
    "model21.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model21.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model21.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model21.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "676tx8WaS1Jy",
    "outputId": "2e341f76-74ad-4bca-94e4-c18733fb8933"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 300 neurons - we will use 6 layers of 50 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes22 = 50\n",
    "\n",
    "# initialize our new model as sequential class\n",
    "model22 = Sequential()\n",
    "\n",
    "# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments\n",
    "model22.add(Dense(nodes22, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'\n",
    "\n",
    "for i in range(5):\n",
    "  model22.add(Dense(nodes22, activation='relu'))\n",
    "\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model22.add(Dense(1))\n",
    "model22.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model22.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model22.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model22.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0NqcaZzJS3oP",
    "outputId": "79362e6d-827c-4a98-d070-28b4f1fd2b5c"
   },
   "outputs": [],
   "source": [
    "# for this NN we will define it with 600 neurons\n",
    "nodes23 = 600\n",
    "\n",
    "model23 = Sequential()\n",
    "model23.add(Dense(nodes23, activation='relu', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "# add one additional layer with one neuron \n",
    "model23.add(Dense(1))\n",
    "# we will use our customized activation function\n",
    "model23.add(Activation(custom_activation))\n",
    "\n",
    "# compile the model\n",
    "model23.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model23.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model23.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XSUdEoiGTBbj",
    "outputId": "90875ed9-cfd3-469b-9bad-3df54e005793"
   },
   "outputs": [],
   "source": [
    "# for the DNN we will continue to use 600 neurons - but instead of a single layer of 600 neurons, we will use 6 layers of 100 neurons each\n",
    "\n",
    "# number of neurons per layer\n",
    "nodes24 = 100\n",
    "model24 = Sequential()\n",
    "\n",
    "model24.add(Dense(nodes24, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model24.add(Dense(nodes24, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model24.add(Dense(1))\n",
    "model24.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model24.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model24.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model24.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4kV2VWnVTGq"
   },
   "source": [
    "New NN with different activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bfaMlxLsVeJS",
    "outputId": "30b3e530-edd0-4430-ed00-b1a0d925350e"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and sigmoid activation\n",
    "nodes26 = 300\n",
    "model26 = Sequential()\n",
    "\n",
    "\n",
    "model26.add(Dense(nodes26, activation='sigmoid', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model26.add(Dense(1))\n",
    "model26.add(Activation(custom_activation))\n",
    "\n",
    "model26.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model26.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model26.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7OGgjkXzVkjh",
    "outputId": "3ec64bd7-4bb7-40a9-d6a8-e302d907b09a"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using Sigmoid activation function\n",
    "nodes27 = 100\n",
    "model27 = Sequential()\n",
    "\n",
    "model27.add(Dense(nodes27, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model27.add(Dense(nodes27, activation='sigmoid'))\n",
    "model27.add(Dense(nodes27, activation='sigmoid'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model27.add(Dense(1))\n",
    "model27.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model27.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model27.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model27.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gIk3XTQ2VnYt",
    "outputId": "503fb273-856f-4e95-8d07-ae50930dfb5a"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, sigmoid, relu\n",
    "nodes27_2 = 100\n",
    "model27_2 = Sequential()\n",
    "\n",
    "model27_2.add(Dense(nodes27_2, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model27_2.add(Dense(nodes27_2, activation='sigmoid'))\n",
    "model27_2.add(Dense(nodes27_2, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model27_2.add(Dense(1))\n",
    "model27_2.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model27_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model27_2.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model27_2.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lp78cBMbVqER",
    "outputId": "ce8f1107-e041-4ad6-e0dd-aea5eff5a3a6"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, relu, relu\n",
    "nodes27_3 = 100\n",
    "model27_3 = Sequential()\n",
    "\n",
    "model27_3.add(Dense(nodes27_3, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model27_3.add(Dense(nodes27_3, activation='relu'))\n",
    "model27_3.add(Dense(nodes27_3, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model27_3.add(Dense(1))\n",
    "model27_3.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model27_3.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model27_3.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model27_3.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ErweliYLVsby",
    "outputId": "33b62766-6365-498e-b3dd-5e43e93d32e2"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using  activation function: relu, relu, sigmoid\n",
    "nodes27_4 = 100\n",
    "model27_4 = Sequential()\n",
    "\n",
    "model27_4.add(Dense(nodes27_4, activation='relu', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model27_4.add(Dense(nodes27_4, activation='relu'))\n",
    "model27_4.add(Dense(nodes27_4, activation='sigmoid'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model27_4.add(Dense(1))\n",
    "model27_4.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model27_4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model27_4.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model27_4.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6dKHbIg3V1CI",
    "outputId": "ff4cf629-b416-44e6-9240-4993d9e634bd"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and tanh activation\n",
    "nodes28 = 300\n",
    "model28 = Sequential()\n",
    "\n",
    "\n",
    "model28.add(Dense(nodes28, activation='tanh', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model28.add(Dense(1))\n",
    "model28.add(Activation(custom_activation))\n",
    "\n",
    "model28.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model28.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model28.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZTXBEY04V3fC",
    "outputId": "bffba288-5b65-4ce5-fda9-ce5eabe85c76"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using tanh activation function\n",
    "nodes29 = 100\n",
    "model29 = Sequential()\n",
    "\n",
    "model29.add(Dense(nodes29, activation='tanh', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model29.add(Dense(nodes29, activation='tanh'))\n",
    "model29.add(Dense(nodes29, activation='tanh'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model29.add(Dense(1))\n",
    "model29.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model29.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model29.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model29.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmWgCIegYtAE"
   },
   "source": [
    "Models with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FYbxjJRWV6RQ",
    "outputId": "1170cdac-46fb-4246-f847-304b5fe283c4"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and rmsprop optimizer\n",
    "nodes30 = 300\n",
    "model30 = Sequential()\n",
    "\n",
    "\n",
    "model30.add(Dense(nodes30, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model30.add(Dense(1))\n",
    "model30.add(Activation(custom_activation))\n",
    "\n",
    "model30.compile(loss='mse', optimizer='rmsprop') \n",
    "\n",
    "# fit the model\n",
    "history = model30.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model30.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5OlhXFWLYxW2",
    "outputId": "4797e761-1c7f-4b93-83a3-96f5c19a4af2"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using relu activation function and rmsprop optimizer\n",
    "nodes31 = 100\n",
    "model31 = Sequential()\n",
    "\n",
    "model31.add(Dense(nodes31, activation='relu', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model31.add(Dense(nodes31, activation='relu'))\n",
    "model31.add(Dense(nodes31, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model31.add(Dense(1))\n",
    "model31.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model31.compile(loss='mse', optimizer='rmsprop') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model31.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model31.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GkO9fiF7Yz11",
    "outputId": "f21eaf9f-f412-4ede-a559-754878bc8b9b"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and momentum (sgd) optimizer\n",
    "nodes32 = 300\n",
    "model32 = Sequential()\n",
    "\n",
    "\n",
    "model32.add(Dense(nodes32, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model32.add(Dense(1))\n",
    "model32.add(Activation(custom_activation))\n",
    "\n",
    "model32.compile(loss='mse', optimizer='sgd') \n",
    "\n",
    "# fit the model\n",
    "history = model32.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model32.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QmYdhG-AY2ee",
    "outputId": "6c8b62a3-77e5-4783-9ebe-33e7f3d968e8"
   },
   "outputs": [],
   "source": [
    "# DNN with 3 layers of 100 neurons, using relu activation function and momentum (sgd) optimizer\n",
    "nodes33 = 100\n",
    "model33 = Sequential()\n",
    "\n",
    "model33.add(Dense(nodes33, activation='relu', input_dim=X2_scaled_train.shape[1])) \n",
    "\n",
    "model33.add(Dense(nodes33, activation='relu'))\n",
    "model33.add(Dense(nodes33, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model33.add(Dense(1))\n",
    "model33.add(Activation(custom_activation))\n",
    "\n",
    "\n",
    "# compile and fit the model\n",
    "model33.compile(loss='mse', optimizer='sgd') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model33.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model33.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKTOeLzCZn_6"
   },
   "source": [
    "Testing different Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ioNYfgosY43J",
    "outputId": "c8c82bd5-52c5-4f4d-ef49-37b9db0c930b"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and adam optimizer and 500 epochs\n",
    "nodes34 = 300\n",
    "model34 = Sequential()\n",
    "\n",
    "\n",
    "model34.add(Dense(nodes34, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model34.add(Dense(1))\n",
    "model34.add(Activation(custom_activation))\n",
    "\n",
    "model34.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model34.fit(X2_scaled_train, y2_train, batch_size=64, epochs=500, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model34.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EcKN7Tr_bWCK",
    "outputId": "c42aac0d-1f01-4102-863f-31e335af6ae7"
   },
   "outputs": [],
   "source": [
    "#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 500 epochs\n",
    "nodes35 = 100\n",
    "model35 = Sequential()\n",
    "\n",
    "model35.add(Dense(nodes35, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model35.add(Dense(nodes35, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model35.add(Dense(1))\n",
    "model35.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model35.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model35.fit(X2_scaled_train, y2_train, batch_size=512, epochs=500, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model35.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I7fNIMBwbYUl",
    "outputId": "d7dac6bc-67b1-4bed-e3ef-62df9a94db22"
   },
   "outputs": [],
   "source": [
    "# SNN with 300 neurons and relu activation and adam optimizer and 1000 epochs\n",
    "nodes36 = 300\n",
    "model36 = Sequential()\n",
    "\n",
    "\n",
    "model36.add(Dense(nodes36, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "model36.add(Dense(1))\n",
    "model36.add(Activation(custom_activation))\n",
    "\n",
    "model36.compile(loss='mse', optimizer='adam') \n",
    "\n",
    "# fit the model\n",
    "history = model36.fit(X2_scaled_train, y2_train, batch_size=64, epochs=1000, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "# print the model parameters\n",
    "print(model36.summary())\n",
    "\n",
    "# plot the model learning history\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('Single NN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gqRKqIYWbaO_",
    "outputId": "a927df86-e7a5-48f1-f538-ada4d8fa0192"
   },
   "outputs": [],
   "source": [
    "#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 1000 epochs\n",
    "nodes37 = 100\n",
    "model37 = Sequential()\n",
    "\n",
    "model37.add(Dense(nodes37, activation='relu', input_dim=X2_scaled_train.shape[1]))\n",
    "\n",
    "for i in range(5):\n",
    "  model37.add(Dense(nodes37, activation='relu'))\n",
    "\n",
    "# output layer is the same as the single NN\n",
    "model37.add(Dense(1))\n",
    "model37.add(Activation(custom_activation))\n",
    "\n",
    "# compile and fit the model\n",
    "model37.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'\n",
    "history = model37.fit(X2_scaled_train, y2_train, batch_size=512, epochs=1000, validation_split=0.2, verbose=2,\n",
    "                          validation_data=(X2_scaled_val, y2_val))\n",
    "print()\n",
    "print(model37.summary())\n",
    "\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)\n",
    "plt.title('DNN Training Evolution')\n",
    "plt.ylabel('Log MSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7zgwe7bbcOn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project2_NNBlack_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
