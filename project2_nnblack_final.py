# -*- coding: utf-8 -*-
"""Project2_NNBlack_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4LIqPuVswj2eI1N5WZXhRPYSkrIlAsA
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()

# Commented out IPython magic to ensure Python compatibility.
# importing packages
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# if running in Google colab - you should upload the Project2_training.csv to the sample_data folder available when you start the notebook

# change the directory to sample_data
# %cd sample_data
# %ls

# reading training file
df = pd.read_csv('Project2_training.csv')

# normalize stock and call prices in relation to the option's strike price
df['Stock Price'] = df['Stock Price']/df['Strike Price']
df['Call Price']  = df['Call Price'] /df['Strike Price']

# split data into training and testing sets
n = 3000
n_train =  (int)(0.8 * n)

# select training set and define independent and dependent variable (call price)
train = df[0:n_train]
X_train = train[['Stock Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free']].values
y_train = train['Call Price'].values

# select testing set and define independent and dependent variable (call price)
test = df[n_train+1:n]
X_test = test[['Stock Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free']].values
y_test = test['Call Price'].values

"""Load Tensor Flow Keras"""

# to build the neural we will use Keras due to its friendly API
# https://keras.io/api/

# importing Keras classes to build the neural networks
from keras.models import Sequential # https://keras.io/api/models/sequential/

# keras provides layers api to construct the neural network
from keras.layers import Dense, Dropout, Activation, LeakyReLU, ReLU #https://keras.io/api/layers/

from keras import backend
def custom_activation(x):
    return backend.exp(x)

from numpy.random import seed
import tensorflow as tf
import random
seed(0)
tf.random.set_seed(0)
random.seed(0)

"""Single Neural Network"""

# for this NN we will define it with 300 neurons
nodes = 300
# initialize the mode as Sequential class - the next steps we will add layers to it 
model = Sequential()

# to add a layer we call the add method from model (Sequential) and pass as argument the Dense class
# https://keras.io/api/layers/core_layers/dense/
model.add(Dense(nodes, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

# since we want to predict a single value we add one additional layer with one neuron 
model.add(Dense(1))
# we will use our customized activation function
model.add(Activation(custom_activation))


# https://keras.io/api/models/model_training_apis/

# compile the model
model.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'

# fit the model
history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Deep Neural Network (DNN)"""

# for the DNN we will continue to use 300 neurons - but instead of a single layer of 300 neurons, we will use 3 layers of 100 neurons each

# number of neurons per layer
nodes2 = 100

# initialize our new model as sequential class
model2 = Sequential()

# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments
model2.add(Dense(nodes2, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

# add second layer of 100 neurons - no need to declare the input dimensions, keras does that automatically
model2.add(Dense(nodes2, activation='relu'))

# add third layer of 100 neurons 
model2.add(Dense(nodes2, activation='relu'))

# output layer is the same as the single NN
model2.add(Dense(1))
model2.add(Activation(custom_activation))


# compile and fit the model
model2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model2.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# for the DNN we will continue to use 300 neurons - we will use 6 layers of 50 neurons each

# number of neurons per layer
nodes2_2 = 50

# initialize our new model as sequential class
model2_2 = Sequential()

# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments
model2_2.add(Dense(nodes2_2, activation='relu', input_dim=X_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

for i in range(5):
  model2_2.add(Dense(nodes2_2, activation='relu'))


# output layer is the same as the single NN
model2_2.add(Dense(1))
model2_2.add(Activation(custom_activation))


# compile and fit the model
model2_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model2_2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model2_2.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""New NN structure with 1200 neurons"""

# for this NN we will define it with 600 neurons
nodes3 = 600

model3 = Sequential()
model3.add(Dense(nodes3, activation='relu', input_dim=X_train.shape[1])) 

# add one additional layer with one neuron 
model3.add(Dense(1))
# we will use our customized activation function
model3.add(Activation(custom_activation))

# compile the model
model3.compile(loss='mse', optimizer='adam') 

# fit the model
history = model3.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model3.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# for the DNN we will continue to use 600 neurons - but instead of a single layer of 600 neurons, we will use 6 layers of 100 neurons each

# number of neurons per layer
nodes4 = 100
model4 = Sequential()

model4.add(Dense(nodes4, activation='relu', input_dim=X_train.shape[1]))

for i in range(5):
  model4.add(Dense(nodes4, activation='relu'))

# output layer is the same as the single NN
model4.add(Dense(1))
model4.add(Activation(custom_activation))

# compile and fit the model
model4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model4.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model4.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""New NN with different activation functions"""

# SNN with 300 neurons and sigmoid activation
nodes6 = 300
model6 = Sequential()


model6.add(Dense(nodes6, activation='sigmoid', input_dim=X_train.shape[1]))

model6.add(Dense(1))
model6.add(Activation(custom_activation))

model6.compile(loss='mse', optimizer='adam') 

# fit the model
history = model6.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model6.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using Sigmoid activation function
nodes7 = 100
model7 = Sequential()

model7.add(Dense(nodes7, activation='sigmoid', input_dim=X_train.shape[1])) 

model7.add(Dense(nodes7, activation='sigmoid'))
model7.add(Dense(nodes7, activation='sigmoid'))

# output layer is the same as the single NN
model7.add(Dense(1))
model7.add(Activation(custom_activation))


# compile and fit the model
model7.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model7.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model7.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, sigmoid, relu
nodes7_2 = 100
model7_2 = Sequential()

model7_2.add(Dense(nodes7_2, activation='sigmoid', input_dim=X_train.shape[1])) 

model7_2.add(Dense(nodes7_2, activation='sigmoid'))
model7_2.add(Dense(nodes7_2, activation='relu'))

# output layer is the same as the single NN
model7_2.add(Dense(1))
model7_2.add(Activation(custom_activation))


# compile and fit the model
model7_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model7_2.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model7_2.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, relu, relu
nodes7_3 = 100
model7_3 = Sequential()

model7_3.add(Dense(nodes7_3, activation='sigmoid', input_dim=X_train.shape[1])) 

model7_3.add(Dense(nodes7_3, activation='relu'))
model7_3.add(Dense(nodes7_3, activation='relu'))

# output layer is the same as the single NN
model7_3.add(Dense(1))
model7_3.add(Activation(custom_activation))


# compile and fit the model
model7_3.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model7_3.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model7_3.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: relu, relu, sigmoid
nodes7_4 = 100
model7_4 = Sequential()

model7_4.add(Dense(nodes7_4, activation='relu', input_dim=X_train.shape[1])) 

model7_4.add(Dense(nodes7_4, activation='relu'))
model7_4.add(Dense(nodes7_4, activation='sigmoid'))

# output layer is the same as the single NN
model7_4.add(Dense(1))
model7_4.add(Activation(custom_activation))


# compile and fit the model
model7_4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model7_4.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model7_4.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and tanh activation
nodes8 = 300
model8 = Sequential()


model8.add(Dense(nodes8, activation='tanh', input_dim=X_train.shape[1]))

model8.add(Dense(1))
model8.add(Activation(custom_activation))

model8.compile(loss='mse', optimizer='adam') 

# fit the model
history = model8.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model8.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using tanh activation function
nodes9 = 100
model9 = Sequential()

model9.add(Dense(nodes9, activation='tanh', input_dim=X_train.shape[1])) 

model9.add(Dense(nodes9, activation='tanh'))
model9.add(Dense(nodes9, activation='tanh'))

# output layer is the same as the single NN
model9.add(Dense(1))
model9.add(Activation(custom_activation))


# compile and fit the model
model9.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model9.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model9.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Models with different Parameters"""

# SNN with 300 neurons and relu activation and rmsprop optimizer
nodes10 = 300
model10 = Sequential()


model10.add(Dense(nodes10, activation='relu', input_dim=X_train.shape[1]))

model10.add(Dense(1))
model10.add(Activation(custom_activation))

model10.compile(loss='mse', optimizer='rmsprop') 

# fit the model
history = model10.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model10.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using relu activation function and rmsprop optimizer
nodes11 = 100
model11 = Sequential()

model11.add(Dense(nodes11, activation='relu', input_dim=X_train.shape[1])) 

model11.add(Dense(nodes11, activation='relu'))
model11.add(Dense(nodes11, activation='relu'))

# output layer is the same as the single NN
model11.add(Dense(1))
model11.add(Activation(custom_activation))


# compile and fit the model
model11.compile(loss='mse', optimizer='rmsprop') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model11.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model11.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and relu activation and momentum (sgd) optimizer
nodes12 = 300
model12 = Sequential()


model12.add(Dense(nodes12, activation='relu', input_dim=X_train.shape[1]))

model12.add(Dense(1))
model12.add(Activation(custom_activation))

model12.compile(loss='mse', optimizer='sgd') 

# fit the model
history = model12.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model12.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using relu activation function and momentum (sgd) optimizer
nodes13 = 100
model13 = Sequential()

model13.add(Dense(nodes13, activation='relu', input_dim=X_train.shape[1])) 

model13.add(Dense(nodes13, activation='relu'))
model13.add(Dense(nodes13, activation='relu'))

# output layer is the same as the single NN
model13.add(Dense(1))
model13.add(Activation(custom_activation))


# compile and fit the model
model13.compile(loss='mse', optimizer='sgd') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model13.fit(X_train, y_train, batch_size=512, epochs=100, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model13.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Testing different Epochs"""

# SNN with 300 neurons and relu activation and adam optimizer and 500 epochs
nodes14 = 300
model14 = Sequential()


model14.add(Dense(nodes14, activation='relu', input_dim=X_train.shape[1]))

model14.add(Dense(1))
model14.add(Activation(custom_activation))

model14.compile(loss='mse', optimizer='adam') 

# fit the model
history = model14.fit(X_train, y_train, batch_size=64, epochs=500, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model14.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 500 epochs
nodes15 = 100
model15 = Sequential()

model15.add(Dense(nodes15, activation='relu', input_dim=X_train.shape[1]))

for i in range(5):
  model15.add(Dense(nodes15, activation='relu'))

# output layer is the same as the single NN
model15.add(Dense(1))
model15.add(Activation(custom_activation))

# compile and fit the model
model15.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model15.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model15.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and relu activation and adam optimizer and 1000 epochs
nodes16 = 300
model16 = Sequential()


model16.add(Dense(nodes16, activation='relu', input_dim=X_train.shape[1]))

model16.add(Dense(1))
model16.add(Activation(custom_activation))

model16.compile(loss='mse', optimizer='adam') 

# fit the model
history = model16.fit(X_train, y_train, batch_size=64, epochs=1000, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
# print the model parameters
print(model16.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 1000 epochs
nodes17 = 100
model17 = Sequential()

model17.add(Dense(nodes17, activation='relu', input_dim=X_train.shape[1]))

for i in range(5):
  model17.add(Dense(nodes17, activation='relu'))

# output layer is the same as the single NN
model17.add(Dense(1))
model17.add(Activation(custom_activation))

# compile and fit the model
model17.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model17.fit(X_train, y_train, batch_size=512, epochs=1000, validation_split=0.1, verbose=2,
                          validation_data=(X_test, y_test))
print()
print(model17.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""# Divide data into Training, Validation and Test set"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from numpy.random import seed
import tensorflow as tf
import random
seed(0)
tf.random.set_seed(0)
random.seed(0)

# Include option price with and without noise in data set splitting for later BS mean error calculation on test set
X2 = df[['Stock Price', 'Risk-free','Volatility','Maturity','Dividends']]
y2 = df[['Call Price']]


#Divide data into training (60%), testing (20%) and validation (20%) sets

# Divide data into training set and test set(note that random seed is set)
X2_train,X2_test,y2_train,y2_test=train_test_split(X2,y2,test_size=0.4,random_state=100)

# Divide training set into training and validation set
X2_train,X2_val,y2_train,y2_val=train_test_split(X2_train,y2_train,test_size=0.5,random_state=100)

# Scale features based on Z-Score
scaler = StandardScaler()
scaler.fit(X2_train)


X2_scaled_train = scaler.transform(X2_train)
X2_scaled_val = scaler.transform(X2_val)
X2_scaled_test = scaler.transform(X2_test)

y2_train = np.asarray(y2_train)
y2_val = np.asarray(y2_val)
y2_test = np.asarray(y2_test)

"""Single Neural Network"""

# for this NN we will define it with 300 neurons
nodes20 = 300
# initialize the mode as Sequential class - the next steps we will add layers to it 
model20 = Sequential()

# to add a layer we call the add method from model (Sequential) and pass as argument the Dense class
# https://keras.io/api/layers/core_layers/dense/
model20.add(Dense(nodes20, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

# since we want to predict a single value we add one additional layer with one neuron 
model20.add(Dense(1))
# we will use our customized activation function
model20.add(Activation(custom_activation))


# https://keras.io/api/models/model_training_apis/

# compile the model
model20.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'

# fit the model
history = model20.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model20.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Deep Neural Network"""

# for the DNN we will continue to use 300 neurons - but instead of a single layer of 300 neurons, we will use 3 layers of 100 neurons each

# number of neurons per layer
nodes21 = 100

# initialize our new model as sequential class
model21 = Sequential()

# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments
model21.add(Dense(nodes21, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

# add second layer of 100 neurons - no need to declare the input dimensions, keras does that automatically
model21.add(Dense(nodes21, activation='relu'))

# add third layer of 100 neurons 
model21.add(Dense(nodes21, activation='relu'))

# output layer is the same as the single NN
model21.add(Dense(1))
model21.add(Activation(custom_activation))


# compile and fit the model
model21.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model21.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model21.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# for the DNN we will continue to use 300 neurons - we will use 6 layers of 50 neurons each

# number of neurons per layer
nodes22 = 50

# initialize our new model as sequential class
model22 = Sequential()

# add the first layer of 100 neurons - for the first layer you must declare the number of features through input_dim arguments
model22.add(Dense(nodes22, activation='relu', input_dim=X2_scaled_train.shape[1])) # activation = 'relu', 'tanh', 'sigmoid'

for i in range(5):
  model22.add(Dense(nodes22, activation='relu'))


# output layer is the same as the single NN
model22.add(Dense(1))
model22.add(Activation(custom_activation))


# compile and fit the model
model22.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model22.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model22.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# for this NN we will define it with 600 neurons
nodes23 = 600

model23 = Sequential()
model23.add(Dense(nodes23, activation='relu', input_dim=X2_scaled_train.shape[1])) 

# add one additional layer with one neuron 
model23.add(Dense(1))
# we will use our customized activation function
model23.add(Activation(custom_activation))

# compile the model
model23.compile(loss='mse', optimizer='adam') 

# fit the model
history = model23.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model23.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# for the DNN we will continue to use 600 neurons - but instead of a single layer of 600 neurons, we will use 6 layers of 100 neurons each

# number of neurons per layer
nodes24 = 100
model24 = Sequential()

model24.add(Dense(nodes24, activation='relu', input_dim=X2_scaled_train.shape[1]))

for i in range(5):
  model24.add(Dense(nodes24, activation='relu'))

# output layer is the same as the single NN
model24.add(Dense(1))
model24.add(Activation(custom_activation))

# compile and fit the model
model24.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model24.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model24.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""New NN with different activation functions

"""

# SNN with 300 neurons and sigmoid activation
nodes26 = 300
model26 = Sequential()


model26.add(Dense(nodes26, activation='sigmoid', input_dim=X2_scaled_train.shape[1]))

model26.add(Dense(1))
model26.add(Activation(custom_activation))

model26.compile(loss='mse', optimizer='adam') 

# fit the model
history = model26.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model26.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using Sigmoid activation function
nodes27 = 100
model27 = Sequential()

model27.add(Dense(nodes27, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) 

model27.add(Dense(nodes27, activation='sigmoid'))
model27.add(Dense(nodes27, activation='sigmoid'))

# output layer is the same as the single NN
model27.add(Dense(1))
model27.add(Activation(custom_activation))


# compile and fit the model
model27.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model27.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model27.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, sigmoid, relu
nodes27_2 = 100
model27_2 = Sequential()

model27_2.add(Dense(nodes27_2, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) 

model27_2.add(Dense(nodes27_2, activation='sigmoid'))
model27_2.add(Dense(nodes27_2, activation='relu'))

# output layer is the same as the single NN
model27_2.add(Dense(1))
model27_2.add(Activation(custom_activation))


# compile and fit the model
model27_2.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model27_2.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model27_2.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: sigmoid, relu, relu
nodes27_3 = 100
model27_3 = Sequential()

model27_3.add(Dense(nodes27_3, activation='sigmoid', input_dim=X2_scaled_train.shape[1])) 

model27_3.add(Dense(nodes27_3, activation='relu'))
model27_3.add(Dense(nodes27_3, activation='relu'))

# output layer is the same as the single NN
model27_3.add(Dense(1))
model27_3.add(Activation(custom_activation))


# compile and fit the model
model27_3.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model27_3.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model27_3.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using  activation function: relu, relu, sigmoid
nodes27_4 = 100
model27_4 = Sequential()

model27_4.add(Dense(nodes27_4, activation='relu', input_dim=X2_scaled_train.shape[1])) 

model27_4.add(Dense(nodes27_4, activation='relu'))
model27_4.add(Dense(nodes27_4, activation='sigmoid'))

# output layer is the same as the single NN
model27_4.add(Dense(1))
model27_4.add(Activation(custom_activation))


# compile and fit the model
model27_4.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model27_4.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model27_4.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and tanh activation
nodes28 = 300
model28 = Sequential()


model28.add(Dense(nodes28, activation='tanh', input_dim=X2_scaled_train.shape[1]))

model28.add(Dense(1))
model28.add(Activation(custom_activation))

model28.compile(loss='mse', optimizer='adam') 

# fit the model
history = model28.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model28.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using tanh activation function
nodes29 = 100
model29 = Sequential()

model29.add(Dense(nodes29, activation='tanh', input_dim=X2_scaled_train.shape[1])) 

model29.add(Dense(nodes29, activation='tanh'))
model29.add(Dense(nodes29, activation='tanh'))

# output layer is the same as the single NN
model29.add(Dense(1))
model29.add(Activation(custom_activation))


# compile and fit the model
model29.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model29.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model29.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Models with different parameters"""

# SNN with 300 neurons and relu activation and rmsprop optimizer
nodes30 = 300
model30 = Sequential()


model30.add(Dense(nodes30, activation='relu', input_dim=X2_scaled_train.shape[1]))

model30.add(Dense(1))
model30.add(Activation(custom_activation))

model30.compile(loss='mse', optimizer='rmsprop') 

# fit the model
history = model30.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model30.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using relu activation function and rmsprop optimizer
nodes31 = 100
model31 = Sequential()

model31.add(Dense(nodes31, activation='relu', input_dim=X2_scaled_train.shape[1])) 

model31.add(Dense(nodes31, activation='relu'))
model31.add(Dense(nodes31, activation='relu'))

# output layer is the same as the single NN
model31.add(Dense(1))
model31.add(Activation(custom_activation))


# compile and fit the model
model31.compile(loss='mse', optimizer='rmsprop') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model31.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model31.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and relu activation and momentum (sgd) optimizer
nodes32 = 300
model32 = Sequential()


model32.add(Dense(nodes32, activation='relu', input_dim=X2_scaled_train.shape[1]))

model32.add(Dense(1))
model32.add(Activation(custom_activation))

model32.compile(loss='mse', optimizer='sgd') 

# fit the model
history = model32.fit(X2_scaled_train, y2_train, batch_size=64, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model32.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# DNN with 3 layers of 100 neurons, using relu activation function and momentum (sgd) optimizer
nodes33 = 100
model33 = Sequential()

model33.add(Dense(nodes33, activation='relu', input_dim=X2_scaled_train.shape[1])) 

model33.add(Dense(nodes33, activation='relu'))
model33.add(Dense(nodes33, activation='relu'))

# output layer is the same as the single NN
model33.add(Dense(1))
model33.add(Activation(custom_activation))


# compile and fit the model
model33.compile(loss='mse', optimizer='sgd') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model33.fit(X2_scaled_train, y2_train, batch_size=512, epochs=100, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model33.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

"""Testing different Epochs"""

# SNN with 300 neurons and relu activation and adam optimizer and 500 epochs
nodes34 = 300
model34 = Sequential()


model34.add(Dense(nodes34, activation='relu', input_dim=X2_scaled_train.shape[1]))

model34.add(Dense(1))
model34.add(Activation(custom_activation))

model34.compile(loss='mse', optimizer='adam') 

# fit the model
history = model34.fit(X2_scaled_train, y2_train, batch_size=64, epochs=500, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model34.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 500 epochs
nodes35 = 100
model35 = Sequential()

model35.add(Dense(nodes35, activation='relu', input_dim=X2_scaled_train.shape[1]))

for i in range(5):
  model35.add(Dense(nodes35, activation='relu'))

# output layer is the same as the single NN
model35.add(Dense(1))
model35.add(Activation(custom_activation))

# compile and fit the model
model35.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model35.fit(X2_scaled_train, y2_train, batch_size=512, epochs=500, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model35.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

# SNN with 300 neurons and relu activation and adam optimizer and 1000 epochs
nodes36 = 300
model36 = Sequential()


model36.add(Dense(nodes36, activation='relu', input_dim=X2_scaled_train.shape[1]))

model36.add(Dense(1))
model36.add(Activation(custom_activation))

model36.compile(loss='mse', optimizer='adam') 

# fit the model
history = model36.fit(X2_scaled_train, y2_train, batch_size=64, epochs=1000, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
# print the model parameters
print(model36.summary())

# plot the model learning history
pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('Single NN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

#DNN with 3 layers, 100 neurons each, relu function, adam optimizer with 1000 epochs
nodes37 = 100
model37 = Sequential()

model37.add(Dense(nodes37, activation='relu', input_dim=X2_scaled_train.shape[1]))

for i in range(5):
  model37.add(Dense(nodes37, activation='relu'))

# output layer is the same as the single NN
model37.add(Dense(1))
model37.add(Activation(custom_activation))

# compile and fit the model
model37.compile(loss='mse', optimizer='adam') # optimizer = 'rmsprop', 'sgd', 'adam'
history = model37.fit(X2_scaled_train, y2_train, batch_size=512, epochs=1000, validation_split=0.2, verbose=2,
                          validation_data=(X2_scaled_val, y2_val))
print()
print(model37.summary())


pd.DataFrame(history.history).plot(figsize=(8,5), logy=True)
plt.title('DNN Training Evolution')
plt.ylabel('Log MSE')
plt.xlabel('Iteration')
plt.grid(True)

